{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM36o281eSfUVLWtGTP46LI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/telnarayanan/HuggingFaceTransformers/blob/main/1_TransfomerModels_Transformers_Architecture.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transfer Learning\n",
        "    * Trained models are usually trained with large amounts of data.\n",
        "    * In these large models, the weights are usually shared along with the model.\n",
        "    * So instead of training from scratch, we can begin from the pre-trained weights, and start fine tuning the model.\n",
        "    * Sometimes pretrained models also come with inherent biases.\n",
        "    * To perform fine-tuning, you first acquire a pre-trained language model, then perform additional training with dataset specific to your task.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tJyKD_1OZ2Wo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Transformer Architectures\n",
        "    * Encoders\n",
        "    * Decoders\n",
        "    * Encoder-Decoder Mechanism\n",
        "\n",
        "* Encoder\n",
        "    * Self-attention\n",
        "    * Bi-directional properties\n",
        "\n",
        "* Decoder\n",
        "    * Masked Self-attention\n",
        "    * Uni-Directional\n",
        "    * Auto-regressive\n",
        "\n",
        "* Encoder-Decoder or Sequence to Sequence Transformer"
      ],
      "metadata": {
        "id": "8FEE4fhIbSKi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Archicture\n",
        "    * The skeleton of the model. The definition of each layer and each operation that happens within the model.\n",
        "\n",
        "* CheckPoints    \n",
        "    * These are the weights that will be loaded into the architecture\n",
        "\n",
        "* Model\n",
        "    * This is an umbrella term, that isn't as precise as 'architecture' or 'checkpoint'; It could refer both\n"
      ],
      "metadata": {
        "id": "nLy_xTNyb87o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Encoder Models\n",
        "    * Encoder only architecture - BERT\n",
        "    * Converts input work into feature tensor, or feature vector\n",
        "\n",
        "* Self-Attention\n",
        "    * Each word in the initial seq, affects every word's representation\n",
        "    * Representation of different words get affected by the presence of other words in the sequence.\n",
        "\n",
        "* Encoder\n",
        "    * Masked Language Modelling\n",
        "     "
      ],
      "metadata": {
        "id": "8oYmzvhTdBiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Decoder Architecture\n",
        "    * Eg. GPT, GPT 2\n",
        "    * Masked Self attention (Words on the rights are masked from the context. It hides the context)\n",
        "    * Useful in Language Generation"
      ],
      "metadata": {
        "id": "8h1eGUSqecTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Sequence to Sequence Architecture / Encoder-Decoder Architecture\n",
        "    * Eg. T5 models.\n",
        "    * BART - (Autoregessive - Output of Encoder is the Input of Decoder)\n"
      ],
      "metadata": {
        "id": "KUNKezcZfdr9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EcXJutm7gJqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}